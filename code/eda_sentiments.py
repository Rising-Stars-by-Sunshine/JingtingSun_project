# -*- coding: utf-8 -*-
"""EDA_Sentiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KeA8I59b6UKooO_-9GKR7mmG2aWw_-Wr
"""

# Cell 1: install
!pip -q install snownlp scikit-learn

# Cell 2: imports & parameters
import os, re
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
from snownlp import SnowNLP
from sklearn.metrics import (
    accuracy_score, precision_recall_fscore_support,
    classification_report, cohen_kappa_score, confusion_matrix
)

# I/O
INPUT_XLSX        = "Newspapers_merged_sorted.xlsx"   # 你的标题数据表
OUT_XLSX          = "EDA_and_basic_sentiment.xlsx"
OUT_PROCESSED_CSV = "titles_sentiment.csv"            # 可公开的派生表（只含标题层）
SAVE_DIR          = "."                               # 输出目录

# 情感阈值（无验证时使用）
POS_THRESH        = 0.55   # ≥ 0.55 → Positive
NEG_THRESH        = 0.45   # ≤ 0.45 → Negative

# （可选）人审文件：TSV 至少包含 title, human_label ∈ {positive, neutral, negative}
# 没有就留空字符串 ""，代码会自动跳过验证
VALIDATION_TSV    = ""     # 例如 "data/validation/headlines_gold.tsv"

# 规则/词表
SHE_REGEX            = r"(她)"  # 标题显式出现“她”
WOMEN_IMPLICIT_REGEX = r"(妇女|女子|女工|女性|妇人|女界|女权|姑娘|女士|小姐)"
EDU_REGEX            = r"(教育|学校|女校|师范|女师|女学|学堂|学生|招生|课程|科目|小学|中学|大学|书院)"
EDITORIAL_HINT_REGEX = r"(社论|评论|评论员|本报论|编者按)"

pd.set_option("display.max_rows", 100)
pd.set_option("display.max_colwidth", 200)

# Cell 3: helpers
def ensure_str(s):
    return "" if not isinstance(s, str) else s.strip()

def snow_score(text: str) -> float:
    """SnowNLP 情感分数 [0,1]；异常/空返回 NaN。"""
    if not isinstance(text, str) or not text.strip():
        return np.nan
    try:
        return float(SnowNLP(text).sentiments)
    except Exception:
        return np.nan

def label_from_thresholds(p: float, pos_thr: float, neg_thr: float) -> str:
    """按阈值把分数映射为 {positive, neutral, negative}。"""
    if pd.isna(p):
        return "neutral"
    if p >= pos_thr:
        return "positive"
    if p <= neg_thr:
        return "negative"
    return "neutral"

def guess_editorial(row) -> int:
    """尝试从列或标题关键词推断是否社论/评论版。"""
    for col in ["Section", "Category", "Page", "Type", "版面", "栏目"]:
        if col in row and isinstance(row[col], str):
            if re.search(EDITORIAL_HINT_REGEX, row[col]):
                return 1
    t = ensure_str(row.get("Title", ""))
    return 1 if re.search(EDITORIAL_HINT_REGEX, t) else 0

def guess_female_creator(row) -> int:
    """简单启发式：作者字段出现女士/小姐/夫人/女史视为 1；否则 0。"""
    for col in ["Creator", "Author", "作者", "撰稿", "记者"]:
        if col in row and isinstance(row[col], str):
            if re.search(r"(女士|小姐|夫人|女史)", row[col]):
                return 1
    return 0

def plot_percent_positive(df_counts, title):
    """输入按年计数表，画每年 Positive 百分比。"""
    if df_counts.empty:
        return
    dfc = df_counts.reindex(columns=["positive","negative","neutral"], fill_value=0)
    totals = dfc.sum(axis=1)
    pos_pct = (dfc['positive'] / totals * 100).fillna(0)

    plt.figure(figsize=(12,4))
    ax = pos_pct.plot(kind='bar', color='C0')
    ax.set_title(title); ax.set_xlabel("Year"); ax.set_ylabel("Positive (%)"); ax.set_ylim(0, 100)
    ax.grid(axis='y', linestyle='--', alpha=0.4)
    for p, val in zip(ax.patches, pos_pct):
        ax.annotate(f"{val:.1f}%", (p.get_x()+p.get_width()/2, val),
                    ha='center', va='bottom', fontsize=9, xytext=(0,3), textcoords='offset points')
    plt.tight_layout(); plt.show()

# Cell 4: load data
df = pd.read_excel(INPUT_XLSX)
if "Title" not in df.columns:
    raise ValueError("Column `Title` not found in the input Excel file.")

df["Title"] = df["Title"].astype(str).str.strip()

# 自动寻找日期列
date_col = "Time" if "Time" in df.columns else ("Date" if "Date" in df.columns else None)
df["date"] = pd.to_datetime(df[date_col], errors="coerce") if date_col else pd.NaT
df["year"] = df["date"].dt.year

print("NOTE: Title-level metadata only. Results describe HEADLINE FRAMING; not full-article arguments.")
df.head(3)

# Cell 5: yearly counts & top-10 titles
year_counts = (
    df["year"].value_counts(dropna=False)
      .sort_index()
      .rename("count")
      .to_frame()
)
if not year_counts.empty:
    plt.figure(figsize=(10,4))
    year_counts["count"].plot(kind="bar")
    plt.title("Count of Titles by Year"); plt.xlabel("Year"); plt.ylabel("Count")
    plt.tight_layout(); plt.show()

print("\n[Top 10 titles overall]")
for i, t in enumerate(df["Title"].head(10).tolist(), 1):
    print(f"{i:>2}. {t}")

# Cell 6: explicit “她” share & examples
df["explicit_she_title"] = df["Title"].str.contains(SHE_REGEX, na=False)
share_overall = float(df["explicit_she_title"].mean())
print(f"[Share] Titles with explicit “她”: {share_overall:.2%}")

she_by_year = (
    df.groupby("year")["explicit_she_title"]
      .mean()
      .rename("share_with_she")
      .to_frame()
)
if not she_by_year.empty:
    plt.figure(figsize=(10,4))
    she_by_year["share_with_she"].plot(kind="bar")
    plt.title("Share of Titles Containing “她” by Year")
    plt.xlabel("Year"); plt.ylabel("Share"); plt.tight_layout(); plt.show()

hits = df[df["explicit_she_title"]].copy().sort_values(["year","Title"], na_position="last")
cols_wanted = ["Title","year","Newspaper Title","Time","SourceFile"]
cols = [c for c in cols_wanted if c in hits.columns]
print("\n[Top 10 titles containing “她”]")
print(hits[cols].head(10).to_string(index=False) if cols else hits["Title"].head(10).to_string(index=False))

# Cell 7: SnowNLP scoring
print("[INFO] Scoring ALL titles with SnowNLP…")
df["sentiment_score"] = [snow_score(t) for t in tqdm(df["Title"].fillna("").tolist())]
df["sentiment_label"] = [label_from_thresholds(p, POS_THRESH, NEG_THRESH) for p in df["sentiment_score"]]

df_she = df[df["explicit_she_title"]].copy()
print("[INFO] Scoring SHE-subset (redundant but explicit)…")
df_she["sentiment_score_she"] = [snow_score(t) for t in tqdm(df_she["Title"].fillna("").tolist())]
df_she["sentiment_label_she"] = [label_from_thresholds(p, POS_THRESH, NEG_THRESH) for p in df_she["sentiment_score_she"]]

df[["Title","sentiment_score","sentiment_label"]].head(5)

# Cell 8: OPTIONAL — human validation & threshold calibration
def calibrate_thresholds(val_df: pd.DataFrame, score_col: str, label_col: str,
                         base_pos=POS_THRESH, base_neg=NEG_THRESH):
    """
    在 0.5±band 的对称区间上网格检索 band（0.00..0.25），优化 {pos,neu,neg} 的 macro-F1。
    返回 (pos_thr, neg_thr, metrics)。
    """
    y_true = val_df[label_col].astype(str).str.lower().map(
        {"positive":"positive","pos":"positive","neutral":"neutral","neg":"negative","negative":"negative"}
    )
    y_scores = val_df[score_col].astype(float)
    mask = ~y_scores.isna() & y_true.isin({"positive","neutral","negative"})
    y_true = y_true[mask]; y_scores = y_scores[mask]
    if y_true.empty:
        print("[WARN] Validation set empty after cleaning; skip calibration.")
        return base_pos, base_neg, {}

    best = (base_pos, base_neg); best_metrics = {"macro_f1": -1}
    for band in np.arange(0.00, 0.26, 0.01):
        pos_thr, neg_thr = 0.5 + band, 0.5 - band
        y_pred = [label_from_thresholds(s, pos_thr, neg_thr) for s in y_scores]
        acc = accuracy_score(y_true, y_pred)
        prec, rec, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=["positive","neutral","negative"], zero_division=0
        )
        macro_f1 = f1.mean()
        if macro_f1 > best_metrics["macro_f1"]:
            best, best_metrics = (pos_thr, neg_thr), {
                "macro_f1": macro_f1, "accuracy": acc,
                "precision": float(prec.mean()), "recall": float(rec.mean())
            }
    return best[0], best[1], best_metrics

if VALIDATION_TSV and Path(VALIDATION_TSV).exists():
    print(f"[INFO] Loading validation TSV: {VALIDATION_TSV}")
    gold = pd.read_csv(VALIDATION_TSV, sep="\t").rename(columns=str.strip)

    # 合并已有分数；否则对验证集单独打分
    tmp = gold.merge(df[["Title","sentiment_score"]], left_on="title", right_on="Title", how="left")
    if "sentiment_score" not in tmp.columns or tmp["sentiment_score"].isna().all():
        print("[INFO] Scoring validation titles (SnowNLP)…")
        tmp["sentiment_score"] = [snow_score(t) for t in tqdm(tmp["title"].astype(str).tolist())]

    # 阈值校准
    pos_thr, neg_thr, metrics = calibrate_thresholds(tmp, "sentiment_score", "human_label")
    print(f"[CALIBRATION] POS≥{pos_thr:.2f}, NEG≤{neg_thr:.2f}, metrics={metrics}")

    # 用最佳阈值重算全体标签
    df["sentiment_label"] = [label_from_thresholds(p, pos_thr, neg_thr) for p in df["sentiment_score"]]
    df_she["sentiment_label_she"] = [label_from_thresholds(p, pos_thr, neg_thr) for p in df_she["sentiment_score_she"]]

    # 报告一致性
    y_true = tmp["human_label"].astype(str).str.lower().map(
        {"positive":"positive","pos":"positive","neutral":"neutral","neg":"negative","negative":"negative"}
    )
    y_pred = [label_from_thresholds(p, pos_thr, neg_thr) for p in tmp["sentiment_score"]]
    print("\n[VALIDATION] Classification report:")
    print(classification_report(y_true, y_pred, labels=["positive","neutral","negative"], digits=3, zero_division=0))
    print("[VALIDATION] Cohen's kappa:", round(cohen_kappa_score(y_true, y_pred), 3))
    print("[VALIDATION] Confusion matrix (rows=true, cols=pred):\n",
          confusion_matrix(y_true, y_pred, labels=["positive","neutral","negative"]))

    # 混淆术语审查（示例）
    audit_terms = ["女史", "女弟子", "伊"]
    flagged = tmp[[any(t in str(x) for t in audit_terms) for x in tmp["title"].astype(str)]]
    if not flagged.empty:
        print("\n[VALIDATION] Confusion-term audit (sample):")
        print(flagged[["title","human_label","sentiment_score"]].head(10).to_string(index=False))
else:
    print("[INFO] No validation file provided. Using default thresholds: POS>=0.55, NEG<=0.45.")

# Cell 9: summaries & yearly breakdowns
def summarize(series: pd.Series) -> pd.Series:
    vc = series.value_counts()
    n   = int(vc.sum())
    pos = int(vc.get("positive", 0))
    neg = int(vc.get("negative", 0))
    neu = int(vc.get("neutral",  0))
    pos_rate = pos / n if n else np.nan
    return pd.Series({"n": n, "positive": pos, "negative": neg, "neutral": neu, "pos_rate": pos_rate})

sum_all = summarize(df["sentiment_label"])
sum_she = summarize(df_she["sentiment_label_she"]) if not df_she.empty else pd.Series(dtype=float)

all_by_year = df.groupby("year")["sentiment_label"].value_counts().unstack(fill_value=0).sort_index()
she_by_year_sent = df_she.groupby("year")["sentiment_label_she"].value_counts().unstack(fill_value=0).sort_index() if not df_she.empty else pd.DataFrame()

print("\n[ALL] Overall sentiment counts/rates:\n", sum_all)
print("\n[SHE] Sentiment for titles with “她”:\n", sum_she)
all_by_year.head(), she_by_year_sent.head()

# Cell 10: charts
if not all_by_year.empty:
    plt.figure(figsize=(12,4))
    all_by_year.reindex(columns=["positive","negative","neutral"], fill_value=0).plot(kind="bar", ax=plt.gca())
    plt.title("All Titles: Sentiment by Year (SnowNLP)")
    plt.xlabel("Year"); plt.ylabel("Count"); plt.tight_layout(); plt.show()

if not she_by_year_sent.empty:
    plt.figure(figsize=(12,4))
    she_by_year_sent.reindex(columns=["positive","negative","neutral"], fill_value=0).plot(kind="bar", ax=plt.gca())
    plt.title("Titles with “她”: Sentiment by Year (SnowNLP)")
    plt.xlabel("Year"); plt.ylabel("Count"); plt.tight_layout(); plt.show()

plot_percent_positive(all_by_year, "All Titles: % Positive by Year (SnowNLP)")
if not she_by_year_sent.empty:
    plot_percent_positive(she_by_year_sent, "Titles with “她”: % Positive by Year (SnowNLP)")

# Cell 11: build & save processed release table (title-level only)

# paper_id: 优先 Newspapaper Title / Paper / SourceFile(去扩展名) / unknown_paper
if "Newspaper Title" in df.columns:
    paper_id = df["Newspaper Title"].astype(str)
elif "Paper" in df.columns:
    paper_id = df["Paper"].astype(str)
else:
    if "SourceFile" in df.columns:
        paper_id = df["SourceFile"].apply(lambda x: Path(str(x)).stem)
    else:
        paper_id = pd.Series(["unknown_paper"] * len(df))

# women_implicit：匹配女性词汇，但排除已标记 explicit_she_title 的
women_implicit = df["Title"].str.contains(WOMEN_IMPLICIT_REGEX, na=False)
women_implicit = np.where(df["explicit_she_title"], False, women_implicit).astype(int)

# education_topic
education_topic = df["Title"].str.contains(EDU_REGEX, na=False).astype(int)

# editorial/female_creator（保守启发式）
editorial = df.apply(guess_editorial, axis=1).astype(int)
female_creator = df.apply(guess_female_creator, axis=1).astype(int)

# headline_len
headline_len = df["Title"].str.len().fillna(0).astype(int)

# phase_1931plus
phase_1931plus = (df["year"].fillna(-1).astype(int).between(1931, 1937)).astype(int)

# cluster_id（如已有则保留，否则 NaN）
cluster_id = df["cluster_id"] if "cluster_id" in df.columns else pd.Series([np.nan]*len(df))

processed = pd.DataFrame({
    "paper_id": paper_id,
    "date": df["date"].dt.strftime("%Y-%m-%d"),
    "year": df["year"],
    "phase_1931plus": phase_1931plus,
    "explicit_she_title": df["explicit_she_title"].astype(int),
    "women_implicit": women_implicit,
    "education_topic": education_topic,
    "editorial": editorial,
    "female_creator": female_creator,
    "headline_len": headline_len,
    "sentiment_score": df["sentiment_score"],
    "sentiment_label": df["sentiment_label"],
    "cluster_id": cluster_id
})

Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)
processed_path = Path(SAVE_DIR) / OUT_PROCESSED_CSV
processed.to_csv(processed_path, index=False, encoding="utf-8-sig")
print(f"[SAVED] Processed title-level table → {processed_path.resolve()}")

with pd.ExcelWriter(Path(SAVE_DIR) / OUT_XLSX) as writer:
    df.to_excel(writer, index=False, sheet_name="all_with_scores")
    if not df_she.empty:
        df_she.to_excel(writer, index=False, sheet_name="she_subset")
    all_by_year.to_excel(writer, sheet_name="all_by_year_counts")
    if not she_by_year_sent.empty:
        she_by_year_sent.to_excel(writer, sheet_name="she_by_year_counts")

processed.head(10)

# Cell 12: OPTIONAL — create a blank validation TSV template
tpl = pd.DataFrame({
    "title": df["Title"].sample(min(60, len(df)), random_state=0).tolist(),
    "human_label": ["" for _ in range(min(60, len(df)))],  # fill with positive/neutral/negative
    "coder1": ["" for _ in range(min(60, len(df)))],
    "coder2": ["" for _ in range(min(60, len(df)))],
    "notes": ["" for _ in range(min(60, len(df)))]
})
tpl_path = Path(SAVE_DIR) / "headlines_gold_TEMPLATE.tsv"
tpl.to_csv(tpl_path, sep="\t", index=False)
print(f"[SAVED] Validation template → {tpl_path.resolve()}")
tpl.head(5)